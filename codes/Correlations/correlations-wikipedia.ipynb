{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation b/w PageRank and Retrievability\n",
    "\n",
    "### Collection: Wikipedia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pickle\n",
    "import csv\n",
    "import scipy\n",
    "import rbo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank_path = './PageRank/pagerank-pageids.tsv'\n",
    "\n",
    "pagerank = {}\n",
    "with open(pagerank_path, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        # pageid -> pagerank_score\n",
    "        pagerank[int(row[0])] = float(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_aol_path = './Retrievability/allrd-pageids-aol.pickle'\n",
    "rd_artificial_path = './Retrievability/allrd-pageids-artificial.pickle'\n",
    "norm_rd_artificial_path = './Retrievability/norm_allrd_pageids_artificial.pickle'\n",
    "\n",
    "with open(rd_aol_path, 'rb') as f:\n",
    "    allrd_aol = pickle.load(f)\n",
    "\n",
    "with open(rd_artificial_path, 'rb') as f:\n",
    "    allrd_artificial = pickle.load(f)\n",
    "    \n",
    "with open(norm_rd_artificial_path, 'rb') as f:\n",
    "    norm_allrd_artificial = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson's Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pearson's Correlation Coefficient calculation\n",
      "\n",
      "For PageRank vs r(d) (artifical queries) for c = 10:\t rho = 0.0260\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (artifical queries) for c = 20:\t rho = 0.0273\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (artifical queries) for c = 30:\t rho = 0.0276\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (artifical queries) for c = 50:\t rho = 0.0280\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (artifical queries) for c = 100:\t rho = 0.0293\t\tp-value = 0.0\n",
      "\n",
      "For PageRank vs normalized r(d) (artifical queries) for c = 10:\t rho = -0.0052\t\tp-value = 4.084338189182967e-33\n",
      "For PageRank vs normalized r(d) (artifical queries) for c = 20:\t rho = -0.0063\t\tp-value = 1.0064916893265469e-53\n",
      "For PageRank vs normalized r(d) (artifical queries) for c = 30:\t rho = -0.0073\t\tp-value = 8.302183401044182e-74\n",
      "For PageRank vs normalized r(d) (artifical queries) for c = 50:\t rho = -0.0086\t\tp-value = 1.4315280237211882e-103\n",
      "For PageRank vs normalized r(d) (artifical queries) for c = 100:\t rho = -0.0102\t\tp-value = 6.550433198816745e-147\n",
      "\n",
      "For PageRank vs r(d) (AOL queries) for c = 10:\t rho = 0.0062\t\tp-value = 1.9849553123603816e-37\n",
      "For PageRank vs r(d) (AOL queries) for c = 20:\t rho = 0.0069\t\tp-value = 8.365112581340849e-53\n",
      "For PageRank vs r(d) (AOL queries) for c = 30:\t rho = 0.0074\t\tp-value = 1.8791275252260142e-64\n",
      "For PageRank vs r(d) (AOL queries) for c = 50:\t rho = 0.0080\t\tp-value = 1.3246122838042389e-79\n",
      "For PageRank vs r(d) (AOL queries) for c = 100:\t rho = 0.0089\t\tp-value = 5.0547795311906354e-104\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPearson's Correlation Coefficient calculation\\n\")\n",
    "\n",
    "for c in sorted(allrd_artificial, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(pagerank) & set(allrd_artificial[c])\n",
    "    pagerank_list, rd_list = [], []\n",
    "    for pageid in sorted(common_pageids):\n",
    "        pagerank_list.append(pagerank[pageid])\n",
    "        rd_list.append(allrd_artificial[c][pageid])\n",
    "    # Pearson's correlation computation\n",
    "    rho, pval = scipy.stats.pearsonr(pagerank_list, rd_list)\n",
    "    print(f'For PageRank vs r(d) (artifical queries) for c = {c.split(\"_\")[-1]}:\\t rho = {rho:.4f}\\t\\tp-value = {pval}')\n",
    "\n",
    "print()\n",
    "for c in sorted(norm_allrd_artificial, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(pagerank) & set(norm_allrd_artificial[c])\n",
    "    pagerank_list, rd_list = [], []\n",
    "    for pageid in sorted(common_pageids):\n",
    "        pagerank_list.append(pagerank[pageid])\n",
    "        rd_list.append(norm_allrd_artificial[c][pageid])\n",
    "    # Pearson's correlation computation\n",
    "    rho, pval = scipy.stats.pearsonr(pagerank_list, rd_list)\n",
    "    print(f'For PageRank vs normalized r(d) (artifical queries) for c = {c.split(\"_\")[-1]}:\\t rho = {rho:.4f}\\t\\tp-value = {pval}')\n",
    "\n",
    "print()\n",
    "for c in sorted(allrd_aol, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(pagerank) & set(allrd_aol[c])\n",
    "    pagerank_list, rd_list = [], []\n",
    "    for pageid in sorted(common_pageids):\n",
    "        pagerank_list.append(pagerank[pageid])\n",
    "        rd_list.append(allrd_aol[c][pageid])\n",
    "    # Pearson's correlation computation\n",
    "    rho, pval = scipy.stats.pearsonr(pagerank_list, rd_list)\n",
    "    print(f'For PageRank vs r(d) (AOL queries) for c = {c.split(\"_\")[-1]}:\\t rho = {rho:.4f}\\t\\tp-value = {pval}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spearman's Rank Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spearman's Correlation Coefficient calculation\n",
      "\n",
      "For PageRank vs r(d) (artifical queries) for c = 10:\t r = 0.1755\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (artifical queries) for c = 20:\t r = 0.2045\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (artifical queries) for c = 30:\t r = 0.2196\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (artifical queries) for c = 50:\t r = 0.2371\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (artifical queries) for c = 100:\t r = 0.2567\t\tp-value = 0.0\n",
      "\n",
      "For PageRank vs normalized r(d) (artifical queries) for c = 10:\t r = -0.1480\t\tp-value = 0.0\n",
      "For PageRank vs normalized r(d) (artifical queries) for c = 20:\t r = -0.1268\t\tp-value = 0.0\n",
      "For PageRank vs normalized r(d) (artifical queries) for c = 30:\t r = -0.1166\t\tp-value = 0.0\n",
      "For PageRank vs normalized r(d) (artifical queries) for c = 50:\t r = -0.1052\t\tp-value = 0.0\n",
      "For PageRank vs normalized r(d) (artifical queries) for c = 100:\t r = -0.0947\t\tp-value = 0.0\n",
      "\n",
      "For PageRank vs r(d) (AOL queries) for c = 10:\t r = 0.0654\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (AOL queries) for c = 20:\t r = 0.0839\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (AOL queries) for c = 30:\t r = 0.0964\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (AOL queries) for c = 50:\t r = 0.1126\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (AOL queries) for c = 100:\t r = 0.1327\t\tp-value = 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSpearman's Correlation Coefficient calculation\\n\")\n",
    "\n",
    "for c in sorted(allrd_artificial, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(pagerank) & set(allrd_artificial[c])\n",
    "    pagerank_list, rd_list = [], []\n",
    "    for pageid in sorted(common_pageids):\n",
    "        pagerank_list.append(pagerank[pageid])\n",
    "        rd_list.append(allrd_artificial[c][pageid])\n",
    "    pagerank_list, rd_list = zip(*sorted(zip(pagerank_list,rd_list), reverse=True))\n",
    "    # Spearman's correlation computation\n",
    "    corr, pval = scipy.stats.spearmanr(pagerank_list, rd_list)\n",
    "    print(f'For PageRank vs r(d) (artifical queries) for c = {c.split(\"_\")[-1]}:\\t r = {corr:.4f}\\t\\tp-value = {pval}')\n",
    "\n",
    "print()\n",
    "for c in sorted(norm_allrd_artificial, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(pagerank) & set(norm_allrd_artificial[c])\n",
    "    pagerank_list, rd_list = [], []\n",
    "    for pageid in sorted(common_pageids):\n",
    "        pagerank_list.append(pagerank[pageid])\n",
    "        rd_list.append(norm_allrd_artificial[c][pageid])\n",
    "    pagerank_list, rd_list = zip(*sorted(zip(pagerank_list,rd_list), reverse=True))\n",
    "    # Spearman's correlation computation\n",
    "    corr, pval = scipy.stats.spearmanr(pagerank_list, rd_list)\n",
    "    print(f'For PageRank vs normalized r(d) (artifical queries) for c = {c.split(\"_\")[-1]}:\\t r = {corr:.4f}\\t\\tp-value = {pval}')\n",
    "\n",
    "print()\n",
    "for c in sorted(allrd_aol, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(pagerank) & set(allrd_aol[c])\n",
    "    pagerank_list, rd_list = [], []\n",
    "    for pageid in sorted(common_pageids):\n",
    "        pagerank_list.append(pagerank[pageid])\n",
    "        rd_list.append(allrd_aol[c][pageid])\n",
    "    pagerank_list, rd_list = zip(*sorted(zip(pagerank_list,rd_list), reverse=True))\n",
    "    # Spearman's correlation computation\n",
    "    corr, pval = scipy.stats.spearmanr(pagerank_list, rd_list)\n",
    "    print(f'For PageRank vs r(d) (AOL queries) for c = {c.split(\"_\")[-1]}:\\t r = {corr:.4f}\\t\\tp-value = {pval}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kendall Rank Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kendall's Correlation Coefficient calculation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:5218: RuntimeWarning: overflow encountered in long_scalars\n",
      "  (2 * xtie * ytie) / m + x0 * y0 / (9 * m * (size - 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For PageRank vs r(d) (artifical queries) for c = 10:\t tau = 0.1223\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (artifical queries) for c = 20:\t tau = 0.1406\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (artifical queries) for c = 30:\t tau = 0.1504\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (artifical queries) for c = 50:\t tau = 0.1618\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (artifical queries) for c = 100:\t tau = 0.1748\t\tp-value = 0.0\n",
      "\n",
      "For PageRank vs normalized r(d) (artifical queries) for c = 10:\t tau = -0.1002\t\tp-value = 0.0\n",
      "For PageRank vs normalized r(d) (artifical queries) for c = 20:\t tau = -0.0856\t\tp-value = 0.0\n",
      "For PageRank vs normalized r(d) (artifical queries) for c = 30:\t tau = -0.0786\t\tp-value = 0.0\n",
      "For PageRank vs normalized r(d) (artifical queries) for c = 50:\t tau = -0.0709\t\tp-value = 0.0\n",
      "For PageRank vs normalized r(d) (artifical queries) for c = 100:\t tau = -0.0638\t\tp-value = 0.0\n",
      "\n",
      "For PageRank vs r(d) (AOL queries) for c = 10:\t tau = 0.0456\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (AOL queries) for c = 20:\t tau = 0.0576\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (AOL queries) for c = 30:\t tau = 0.0658\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (AOL queries) for c = 50:\t tau = 0.0764\t\tp-value = 0.0\n",
      "For PageRank vs r(d) (AOL queries) for c = 100:\t tau = 0.0896\t\tp-value = 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nKendall's Correlation Coefficient calculation\\n\")\n",
    "\n",
    "for c in sorted(allrd_artificial, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(pagerank) & set(allrd_artificial[c])\n",
    "    pagerank_list, rd_list = [], []\n",
    "    for pageid in sorted(common_pageids):\n",
    "        pagerank_list.append(pagerank[pageid])\n",
    "        rd_list.append(allrd_artificial[c][pageid])\n",
    "    pagerank_list, rd_list = zip(*sorted(zip(pagerank_list,rd_list), reverse=True))\n",
    "    # Kendall's correlation computation\n",
    "    corr, pval = scipy.stats.kendalltau(pagerank_list, rd_list)\n",
    "    print(f'For PageRank vs r(d) (artifical queries) for c = {c.split(\"_\")[-1]}:\\t tau = {corr:.4f}\\t\\tp-value = {pval}')\n",
    "\n",
    "print()\n",
    "for c in sorted(norm_allrd_artificial, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(pagerank) & set(norm_allrd_artificial[c])\n",
    "    pagerank_list, rd_list = [], []\n",
    "    for pageid in sorted(common_pageids):\n",
    "        pagerank_list.append(pagerank[pageid])\n",
    "        rd_list.append(norm_allrd_artificial[c][pageid])\n",
    "    pagerank_list, rd_list = zip(*sorted(zip(pagerank_list,rd_list), reverse=True))\n",
    "    # Kendall's correlation computation\n",
    "    corr, pval = scipy.stats.kendalltau(pagerank_list, rd_list)\n",
    "    print(f'For PageRank vs normalized r(d) (artifical queries) for c = {c.split(\"_\")[-1]}:\\t tau = {corr:.4f}\\t\\tp-value = {pval}')\n",
    "\n",
    "print()\n",
    "for c in sorted(allrd_aol, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(pagerank) & set(allrd_aol[c])\n",
    "    pagerank_list, rd_list = [], []\n",
    "    for pageid in sorted(common_pageids):\n",
    "        pagerank_list.append(pagerank[pageid])\n",
    "        rd_list.append(allrd_aol[c][pageid])\n",
    "    pagerank_list, rd_list = zip(*sorted(zip(pagerank_list,rd_list), reverse=True))\n",
    "    # Kendall's correlation computation\n",
    "    corr, pval = scipy.stats.kendalltau(pagerank_list, rd_list)\n",
    "    print(f'For PageRank vs r(d) (AOL queries) for c = {c.split(\"_\")[-1]}:\\t tau = {corr:.4f}\\t\\tp-value = {pval}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank Biased Overlap (RBO) measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBO measure calculation\n",
      "\n",
      "For PageRank vs r(d) (artifical queries) for c = 10:\t\t rbo = 0.5568\n",
      "For PageRank vs r(d) (artifical queries) for c = 20:\t\t rbo = 0.5627\n",
      "For PageRank vs r(d) (artifical queries) for c = 30:\t\t rbo = 0.5661\n",
      "For PageRank vs r(d) (artifical queries) for c = 50:\t\t rbo = 0.5702\n",
      "For PageRank vs r(d) (artifical queries) for c = 100:\t\t rbo = 0.5753\n",
      "\n",
      "For PageRank vs Normalized r(d) (artifical queries) for c = 10:\t\t rbo = 0.4770\n",
      "For PageRank vs Normalized r(d) (artifical queries) for c = 20:\t\t rbo = 0.4817\n",
      "For PageRank vs Normalized r(d) (artifical queries) for c = 30:\t\t rbo = 0.4840\n",
      "For PageRank vs Normalized r(d) (artifical queries) for c = 50:\t\t rbo = 0.4863\n",
      "For PageRank vs Normalized r(d) (artifical queries) for c = 100:\t\t rbo = 0.4879\n",
      "\n",
      "For PageRank vs r(d) (AOL queries) for c = 10:\t\t rbo = 0.5228\n",
      "For PageRank vs r(d) (AOL queries) for c = 20:\t\t rbo = 0.5255\n",
      "For PageRank vs r(d) (AOL queries) for c = 30:\t\t rbo = 0.5275\n",
      "For PageRank vs r(d) (AOL queries) for c = 50:\t\t rbo = 0.5305\n",
      "For PageRank vs r(d) (AOL queries) for c = 100:\t\t rbo = 0.5343\n"
     ]
    }
   ],
   "source": [
    "print(\"RBO measure calculation\\n\")\n",
    "\n",
    "for c in sorted(allrd_artificial, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(pagerank) & set(allrd_artificial[c])\n",
    "    pagerank_list = [pageid for pageid,_ in sorted(pagerank.items(), key=lambda x: x[1], reverse=True) if pageid in common_pageids]\n",
    "    rd_list = [pageid for pageid,_ in sorted(dict(allrd_artificial[c]).items(), key=lambda x: x[1], reverse=True) if pageid in common_pageids]\n",
    "    # RBO measure computation\n",
    "    rbo_measure = rbo.RankingSimilarity(pagerank_list, rd_list).rbo()\n",
    "    print(f'For PageRank vs r(d) (artifical queries) for c = {c.split(\"_\")[-1]}:\\t\\t rbo = {rbo_measure:.4f}')\n",
    "\n",
    "print()\n",
    "for c in sorted(norm_allrd_artificial, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(pagerank) & set(norm_allrd_artificial[c])\n",
    "    pagerank_list = [pageid for pageid,_ in sorted(pagerank.items(), key=lambda x: x[1], reverse=True) if pageid in common_pageids]\n",
    "    rd_list = [pageid for pageid,_ in sorted(dict(norm_allrd_artificial[c]).items(), key=lambda x: x[1], reverse=True) if pageid in common_pageids]\n",
    "    # RBO measure computation\n",
    "    rbo_measure = rbo.RankingSimilarity(pagerank_list, rd_list).rbo()\n",
    "    print(f'For PageRank vs Normalized r(d) (artifical queries) for c = {c.split(\"_\")[-1]}:\\t\\t rbo = {rbo_measure:.4f}')\n",
    "\n",
    "print()\n",
    "for c in sorted(allrd_aol, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(pagerank) & set(allrd_aol[c])\n",
    "    pagerank_list = [pageid for pageid,_ in sorted(pagerank.items(), key=lambda x: x[1], reverse=True) if pageid in common_pageids]\n",
    "    rd_list = [pageid for pageid,_ in sorted(dict(allrd_aol[c]).items(), key=lambda x: x[1], reverse=True) if pageid in common_pageids]\n",
    "    # RBO measure computation\n",
    "    rbo_measure = rbo.RankingSimilarity(pagerank_list, rd_list).rbo()\n",
    "    print(f'For PageRank vs r(d) (AOL queries) for c = {c.split(\"_\")[-1]}:\\t\\t rbo = {rbo_measure:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation b/w Retrievability on Real Queries vs Artificial Queries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pickle\n",
    "import csv\n",
    "import scipy\n",
    "import rbo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_aol_path = './Retrievability/allrd-pageids-aol.pickle'\n",
    "rd_artificial_path = './Retrievability/allrd-pageids-artificial.pickle'\n",
    "norm_rd_artificial_path = './Retrievability/norm_allrd_pageids_artificial.pickle'\n",
    "\n",
    "with open(rd_aol_path, 'rb') as f:\n",
    "    allrd_aol = pickle.load(f)\n",
    "\n",
    "with open(rd_artificial_path, 'rb') as f:\n",
    "    allrd_artificial = pickle.load(f)\n",
    "    \n",
    "with open(norm_rd_artificial_path, 'rb') as f:\n",
    "    norm_allrd_artificial = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson's Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pearson's Correlation Coefficient calculation\n",
      "\n",
      "For AOL r(d) vs Artificial r(d) for c = 10:\t r = 0.1695\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial r(d) for c = 20:\t r = 0.2080\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial r(d) for c = 30:\t r = 0.2318\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial r(d) for c = 50:\t r = 0.2631\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial r(d) for c = 100:\t r = 0.3065\t\tp-value = 0.0\n",
      "\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 10:\t r = 0.0514\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 20:\t r = 0.0578\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 30:\t r = 0.0593\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 50:\t r = 0.0608\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 100:\t r = 0.0626\t\tp-value = 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPearson's Correlation Coefficient calculation\\n\")\n",
    "\n",
    "for c in sorted(allrd_artificial, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(allrd_aol[c]) & set(allrd_artificial[c])\n",
    "    aol_rd_list, artificial_rd_list = [], []\n",
    "    for pageid in sorted(common_pageids):\n",
    "        aol_rd_list.append(allrd_aol[c][pageid])\n",
    "        artificial_rd_list.append(allrd_artificial[c][pageid])\n",
    "    # Pearson's correlation computation\n",
    "    rho, pval = scipy.stats.pearsonr(aol_rd_list, artificial_rd_list)\n",
    "    print(f'For AOL r(d) vs Artificial r(d) for c = {c.split(\"_\")[-1]}:\\t r = {rho:.4f}\\t\\tp-value = {pval}')\n",
    "\n",
    "print()\n",
    "for c in sorted(norm_allrd_artificial, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(allrd_aol[c]) & set(norm_allrd_artificial[c])\n",
    "    aol_rd_list, artificial_rd_list = [], []\n",
    "    for pageid in sorted(common_pageids):\n",
    "        aol_rd_list.append(allrd_aol[c][pageid])\n",
    "        artificial_rd_list.append(norm_allrd_artificial[c][pageid])\n",
    "    # Pearson's correlation computation\n",
    "    rho, pval = scipy.stats.pearsonr(aol_rd_list, artificial_rd_list)\n",
    "    print(f'For AOL r(d) vs Artificial \"Normalized\" r(d) for c = {c.split(\"_\")[-1]}:\\t r = {rho:.4f}\\t\\tp-value = {pval}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spearman's Rank Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spearman's Correlation Coefficient calculation\n",
      "\n",
      "For AOL r(d) vs Artificial r(d) for c = 10:\t rho = 0.3066\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial r(d) for c = 20:\t rho = 0.3938\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial r(d) for c = 30:\t rho = 0.4482\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial r(d) for c = 50:\t rho = 0.5140\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial r(d) for c = 100:\t rho = 0.5893\t\tp-value = 0.0\n",
      "\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 10:\t rho = 0.0856\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 20:\t rho = 0.0908\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 30:\t rho = 0.0907\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 50:\t rho = 0.0921\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 100:\t rho = 0.1034\t\tp-value = 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSpearman's Correlation Coefficient calculation\\n\")\n",
    "\n",
    "for c in sorted(allrd_artificial, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(allrd_aol[c]) & set(allrd_artificial[c])\n",
    "    aol_rd_list, artificial_rd_list = [], []\n",
    "    for pageid in sorted(common_pageids):\n",
    "        aol_rd_list.append(allrd_aol[c][pageid])\n",
    "        artificial_rd_list.append(allrd_artificial[c][pageid])\n",
    "    # Spearman's correlation computation\n",
    "    rho, pval = scipy.stats.spearmanr(aol_rd_list, artificial_rd_list)\n",
    "    print(f'For AOL r(d) vs Artificial r(d) for c = {c.split(\"_\")[-1]}:\\t rho = {rho:.4f}\\t\\tp-value = {pval}')\n",
    "\n",
    "print()\n",
    "for c in sorted(norm_allrd_artificial, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(allrd_aol[c]) & set(norm_allrd_artificial[c])\n",
    "    aol_rd_list, artificial_rd_list = [], []\n",
    "    for pageid in sorted(common_pageids):\n",
    "        aol_rd_list.append(allrd_aol[c][pageid])\n",
    "        artificial_rd_list.append(norm_allrd_artificial[c][pageid])\n",
    "    # Spearman's correlation computation\n",
    "    rho, pval = scipy.stats.spearmanr(aol_rd_list, artificial_rd_list)\n",
    "    print(f'For AOL r(d) vs Artificial \"Normalized\" r(d) for c = {c.split(\"_\")[-1]}:\\t rho = {rho:.4f}\\t\\tp-value = {pval}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kendall Rank Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kendall's Correlation Coefficient calculation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:5218: RuntimeWarning: overflow encountered in long_scalars\n",
      "  (2 * xtie * ytie) / m + x0 * y0 / (9 * m * (size - 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For AOL r(d) vs Artificial r(d) for c = 10:\t tau = 0.2208\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial r(d) for c = 20:\t tau = 0.2785\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial r(d) for c = 30:\t tau = 0.3157\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial r(d) for c = 50:\t tau = 0.3625\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial r(d) for c = 100:\t tau = 0.4194\t\tp-value = 0.0\n",
      "\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 10:\t tau = 0.0593\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 20:\t tau = 0.0620\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 30:\t tau = 0.0614\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 50:\t tau = 0.0620\t\tp-value = 0.0\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 100:\t tau = 0.0695\t\tp-value = 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nKendall's Correlation Coefficient calculation\\n\")\n",
    "\n",
    "for c in sorted(allrd_artificial, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(allrd_aol[c]) & set(allrd_artificial[c])\n",
    "    aol_rd_list, artificial_rd_list = [], []\n",
    "    for pageid in sorted(common_pageids):\n",
    "        aol_rd_list.append(allrd_aol[c][pageid])\n",
    "        artificial_rd_list.append(allrd_artificial[c][pageid])\n",
    "    aol_rd_list, artificial_rd_list = zip(*sorted(zip(aol_rd_list,artificial_rd_list), reverse=True))\n",
    "    # Kendall's correlation computation\n",
    "    rho, pval = scipy.stats.kendalltau(aol_rd_list, artificial_rd_list)\n",
    "    print(f'For AOL r(d) vs Artificial r(d) for c = {c.split(\"_\")[-1]}:\\t tau = {rho:.4f}\\t\\tp-value = {pval}')\n",
    "\n",
    "print()\n",
    "for c in sorted(norm_allrd_artificial, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(allrd_aol[c]) & set(norm_allrd_artificial[c])\n",
    "    aol_rd_list, artificial_rd_list = [], []\n",
    "    for pageid in sorted(common_pageids):\n",
    "        aol_rd_list.append(allrd_aol[c][pageid])\n",
    "        artificial_rd_list.append(norm_allrd_artificial[c][pageid])\n",
    "    aol_rd_list, artificial_rd_list = zip(*sorted(zip(aol_rd_list,artificial_rd_list), reverse=True))\n",
    "    # Kendall's correlation computation\n",
    "    rho, pval = scipy.stats.kendalltau(aol_rd_list, artificial_rd_list)\n",
    "    print(f'For AOL r(d) vs Artificial \"Normalized\" r(d) for c = {c.split(\"_\")[-1]}:\\t tau = {rho:.4f}\\t\\tp-value = {pval}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank Biased Overlap (RBO) measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBO measure calculation\n",
      "\n",
      "For AOL r(d) vs Artificial r(d) for c = 10:\t rbo = 0.5898\n",
      "For AOL r(d) vs Artificial r(d) for c = 20:\t rbo = 0.6106\n",
      "For AOL r(d) vs Artificial r(d) for c = 30:\t rbo = 0.6235\n",
      "For AOL r(d) vs Artificial r(d) for c = 50:\t rbo = 0.6397\n",
      "For AOL r(d) vs Artificial r(d) for c = 100:\t rbo = 0.6594\n",
      "\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 10:\t rbo = 0.5242\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 20:\t rbo = 0.5253\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 30:\t rbo = 0.5253\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 50:\t rbo = 0.5255\n",
      "For AOL r(d) vs Artificial \"Normalized\" r(d) for c = 100:\t rbo = 0.5276\n"
     ]
    }
   ],
   "source": [
    "print(\"RBO measure calculation\\n\")\n",
    "\n",
    "for c in sorted(allrd_artificial, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(allrd_aol[c]) & set(allrd_artificial[c])\n",
    "    artificial_rd_list = [pageid for pageid,_ in sorted(dict(allrd_artificial[c]).items(), key=lambda x: x[1], reverse=True) if pageid in common_pageids]\n",
    "    aol_rd_list = [pageid for pageid,_ in sorted(dict(allrd_aol[c]).items(), key=lambda x: x[1], reverse=True) if pageid in common_pageids]\n",
    "    # RBO similarity measure computation\n",
    "    rbo_measure = rbo.RankingSimilarity(aol_rd_list, artificial_rd_list).rbo()\n",
    "    print(f'For AOL r(d) vs Artificial r(d) for c = {c.split(\"_\")[-1]}:\\t rbo = {rbo_measure:.4f}')\n",
    "\n",
    "print()\n",
    "for c in sorted(norm_allrd_artificial, key=lambda x: int(x.split('_')[-1])):\n",
    "    common_pageids = set(allrd_aol[c]) & set(norm_allrd_artificial[c])\n",
    "    artificial_rd_list = [pageid for pageid,_ in sorted(dict(norm_allrd_artificial[c]).items(), key=lambda x: x[1], reverse=True) if pageid in common_pageids]\n",
    "    aol_rd_list = [pageid for pageid,_ in sorted(dict(allrd_aol[c]).items(), key=lambda x: x[1], reverse=True) if pageid in common_pageids]\n",
    "    # RBO similarity measure computation\n",
    "    rbo_measure = rbo.RankingSimilarity(aol_rd_list, artificial_rd_list).rbo()\n",
    "    print(f'For AOL r(d) vs Artificial \"Normalized\" r(d) for c = {c.split(\"_\")[-1]}:\\t rbo = {rbo_measure:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
